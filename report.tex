% Jacob Sánchez Pérez (jacob@san.contact)
% Data Science (CO3722)
% University of Central Lancashire

\documentclass[a4paper,12pt]{article}

% Links
\usepackage{hyperref}

% Code
\usepackage{minted}

% Referencing and more
\usepackage[british]{babel}
\usepackage{csquotes}
\usepackage[style=apa,backend=biber]{biblatex}

% Images
\usepackage{graphicx}
\graphicspath{ {./images/} }

% Graphs
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\usepgfplotslibrary{external}
\tikzexternalize

\title{Predicting Diseases with Data Science}
\author{Jacob Sánchez\\ University of Central Lancashire\\\texttt{jsanchez-perez@uclan.ac.uk}}
\date{}

\addbibresource{references.bib}

\begin{document}

\maketitle


\section{Introduction}

% PROMPT
% This section will introduce your project, your business, specific Use Case and potential source of a dataset for analysis.

% NOTES
% The business I have chosen is healthcare

Data science applies different methods, both qualitative and quantitative, in order to solve relevant problems and make predictions \parencite[78]{Waller2013}.
One industry that can benefit from the applications of data science is the healthcare industry.
According to the \textcite{oecd2010health}, improving productivity in the healthcare sector translates to public spending savings.
McKinsey estimated that data analytics can reduce healthcare expenses by \$300 B to \$450 B per year in the U.S. alone \parencite{Groves2013}.
One way of improving healthcare is to utilise data science to process the massive amounts of electronic health records that exist \parencite{Dalianis2015}.
Data collected in healthcare can come in the form of physician notes, medical records, patient scans, and patient sensor data (such as wearables) \parencite{Adam2017}.
However, obtaining said data may present several challenges due to patient privacy. 

Areas in healthcare that could have the most impact from the application of data science include early detection of diseases, precision medicine, optimisation of workflows, value-based healthcare, infection prevention and control, and clinical research \parencite[9]{Consoli2019}.
This report is centred around the application of data science to predict diseases.
It will review in detail the process of data analysis, from locating a data source, processing the data, and finally visualising and interpreting the results.

\section{Critical Discussion of the Application}

% PROMPT
% This section will relate specifically to your business and Use Case and their approach or potential application for the use of data science.
% This will cover the challenge of acquiring or using specific data sources for your Use Case, approaches to data processing, description of proposed machine learning  algorithms and their design, opportunities for data visualisation and the methods for analysing/interpreting the data required to gain business competitive advantage.

\subsection{Data sourcing}

\subsubsection{Privacy concerns}

There exist several limitations to the data freely available for analysis.
As \textcite[2]{Dalianis2015} points out, this is mainly due to the existence of sensitive data about patients in medical records.
A cross-sectional survey found that the two biggest perceived obstacles towards obtaining healthcare data were conflicts of laws and data standardization \parencite{Kim2019}.
Regulatory requirements, namely the General Data Privacy Regulation (GDPR) in the European Union and the Health Insurance Portability and Accountability Act (HIPAA) in the U.S., protect data that can be used to identify a patient \parencite{Iyengar2018}.
GDPR requires explicit consent of the subject to process personal data. Obtaining said consent results impractical in big data analytics, where it is necessary to process several terabytes worth of data \parencite{Hintze2018}.

However, there are other criteria which can allow data to be processed for purposes such as research and analysis.
This criteria calls for \textquote{appropiate safeguards} to be in place, which includes encryption and pseudonymisation \parencite[151]{Hintze2018}.
Pseudonymisation is a way to de-identify information, and consists of replacing or removing direct identifiers, such as names, phone numbers, and other data points that could be directly attributed to a particular subject \parencite[146-147]{Hintze2018}.
Due to the large amounts of data present in health records, de-identification should ideally be automated.
\textcite{Dernoncourt2016} described an automated de-identification technique using neural networks, that achieved 97\%+ precision on different datasets.
However, no method is perfect and this is still a limiting factor in healthcare data availability.
Furthermore, some debate exists about the need for strict de-identification, and how it can affect the availability of data.
\textcite[2]{Shin2018} argues that de-identification can lead to a distortion and loss of detailed data, which can greatly impact the results of a study.
Likewise, \textcite{Kim2019} revealed that almost half (45.8\%) of all participants in a survey thought that a revision in legislation was necessary to reduce obstacles when using healthcare data.
However, it is important to remember that big data analytics can still have consequences for individuals.
Some de-identification methods are vulnerable to correlation attacks, which can identify individuals through indirect identifiers \parencite{Abouelmehdi2018}.
As \textcite{Abouelmehdi2018} points out, analytics should verify privacy agreements are respected, and sensitive information should remain private.

\subsubsection{Processing concerns}

In addition to the legal issues, there is another major obstacle when using healthcare data.
This comes in the form of data processing problems, specially with clinical notes, since these can contain misspelled words, medical jargon, and non-standard words \parencite{Dalianis2015}.
In addition, data is oftentimes collected from systems with different and incompatible formats \parencite[34]{Consoli2019}.
Therefore some amount of preprocessing will need to take place before analysing the data.


\subsubsection{Healthcare data sources}

Despite the many difficulties that exist in making healthcare data ready for research purposes, some de-identified datasets have been made available online.
\textcite{Dalianis2015} mentions a few, including the \textit{i2b2} corpus\footnote{Nowadays known as \textit{n2c2}}, the \textit{CMC} corpus, the \textit{MIMIC II} database, and the \textit{THIN} database. \textit{MIMIC III} \parencite{Johnson2016} is another database available for research.
Finally, some governments partake in releasing healthcare datasets for research, such as the \textit{Open Government Initiative} of the United States government \parencite{Marc2015}, which operates the website HealthData.gov\footnote{\url{https://healthdata.gov}} for this purpose.

\subsection{Data processing techniques}

De-identification techniques will not be covered here, since all of the data that can be obtained will have been de-identified beforehand.
After de-identification, one of the biggest challenges with healthcare data is making sense of it.
In particular, some techniques that might improve the quality of free-text data may include detection of negation and spelling mistakes, abbreviation normalisation, and named entity recognition \parencite{Dalianis2015}.

\subsection{Predictive Algorithms}


\textcite{Jothi2015} outlined some of the classification algorithms commonly used in healthcare data analysis, briefly described here:

\begin{description}
    \item[Decision Tree] \textquote[{\cite[70]{Mueller2017}}]{They learn a hierarchy of if/else questions, leading to a decision}.
    \item[K-nearest Neighbour] Stores the training dataset, then to make a prediction it \textquote[{\cite[35]{Mueller2017}}]{finds the closest data points in the training dataset}.
    \item[Logistic Regression] Statistical model that \textquote{describes the relationship between a qualitative dependent variable and an independent variable}, making binary predictions \parencite{Nick2007}.
    \item[Bayesian Classifier] Each feature is analysed individually, then, per-class statistics are collected for each feature. 
    To make predictions, the \textquote[{\cite[69]{Mueller2017}}]{data point is compared to the statistics for each of the classes,
and the best matching class is predicted}.
    \item[Support Vector] It learns the importance of each training data point to \textquote[{\cite[98]{Mueller2017}}]{represent the decision boundary between two classes}.
    From those, only the ones at the border between classes are necessary to define the decision boundary, these are the support vectors.
    Classification is made based on the distances to the support vectors \parencite[98]{Mueller2017}.
\end{description}

\subsection{Training and Testing}

\textcite[39]{Consoli2019} suggests using the \(k\)-fold cross-validation approach, in which the dataset is split into \(k\) parts, of which \(k-1\) parts are used to train the model, and the last one is used to test it, rotating in such a way that every part is used to test it once. 

% ~450 words remaining

\section{Critical Evaluation of the case for or against uptake of such Technology}

% PROMPT
% You will evaluate the benefits or otherwise, including the challenges of applying machine learning algorithms, maintenance and ethical considerations of a data science model for business insight.
% ~400 words


\section{Conclusions and Recommendations}
% ~150 words
\pagebreak
\printbibliography

\end{document}
